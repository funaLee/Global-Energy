{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 05: Phase 2 - Hyperparameter Tuning\n",
                "\n",
                "**Mục tiêu:**\n",
                "- Tối ưu hóa tham số cho 2 mô hình đã chọn: **Linear Regression (Ridge)** và **XGBoost**.\n",
                "- Khắc phục điểm yếu cụ thể của từng mô hình:\n",
                "    - **Ridge**: Tăng độ ổn định (Stability) bằng cách tìm `alpha` tối ưu để giảm nhiễu.\n",
                "    - **XGBoost**: Giảm hiện tượng học vẹt (Overfitting) bằng cách giới hạn độ sâu `max_depth` và tốc độ học `learning_rate`.\n",
                "- Sử dụng **Time-Series Cross Validation** để đảm bảo quá trình tuning không vi phạm nguyên tắc nhân quả (không dùng tương lai đoán quá khứ).\n",
                "\n",
                "**Input:**\n",
                "- `data/processed/lr_final_prep.csv`: Dữ liệu cho Ridge (One-Hot Encoded).\n",
                "- `data/processed/xgb_final_prep.csv`: Dữ liệu cho XGBoost (Ordinal Encoded).\n",
                "- `data/processed/common_preprocessed.csv`: Dữ liệu metadata (Year/Entity).\n",
                "\n",
                "**Output:**\n",
                "- Bộ tham số tối ưu (Best Params) cho Ridge và XGBoost.\n",
                "- Đánh giá hiệu suất sau khi Tuning (so với Baseline)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.linear_model import Ridge\n",
                "from xgboost import XGBRegressor\n",
                "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
                "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "pd.set_option('display.max_columns', 50)\n",
                "plt.style.use('seaborn-v0_8')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Chuẩn bị Dữ liệu & Hàm hỗ trợ\n",
                "Chúng ta cần load dữ liệu riêng biệt cho LR và XGBoost vì chúng yêu cầu tiền xử lý khác nhau."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_and_prepare_data(filepath, target_col='Value_co2_emissions_kt_by_country'):\n",
                "    \"\"\"\n",
                "    Hàm load dữ liệu và tách X, y, Year để phục vụ TimeSeriesSplit.\n",
                "    \"\"\"\n",
                "    print(f\"Đang tải dữ liệu từ: {filepath}\")\n",
                "    df = pd.read_csv(filepath)\n",
                "    \n",
                "    # Load common data de lay Year nheu trong file processed khong co\n",
                "    df_common = pd.read_csv('../data/processed/common_preprocessed.csv')\n",
                "    common_idx = df.index.intersection(df_common.index)\n",
                "    \n",
                "    df = df.loc[common_idx]\n",
                "    df_year = df_common.loc[common_idx, 'Year']\n",
                "    \n",
                "    # Tách Features và Target\n",
                "    drop_cols = [target_col, 'Year', 'Entity'] # Entity co the da duoc encode hoac drop\n",
                "    drop_cols = [c for c in drop_cols if c in df.columns]\n",
                "    \n",
                "    X = df.drop(columns=drop_cols)\n",
                "    y = df[target_col]\n",
                "    \n",
                "    # Sort theo Year để đảm bảo TimeSeriesSplit hoạt động đúng\n",
                "    # (Cần reset index để merge lại với series Year)\n",
                "    X['Year_Meta'] = df_year.values\n",
                "    X = X.sort_values('Year_Meta')\n",
                "    y = y.loc[X.index]\n",
                "    \n",
                "    # Tách cột Year ra để dùng cho CV splitter, không đưa vào model train\n",
                "    years = X['Year_Meta'].values\n",
                "    X = X.drop(columns=['Year_Meta'])\n",
                "    \n",
                "    print(f\"Kích thước dữ liệu: {X.shape}\")\n",
                "    return X, y, years\n",
                "\n",
                "# Custom Scorer: Negative RMSE (GridSearch can maximize nen can lay so am)\n",
                "rmse_scorer = make_scorer(mean_squared_error, squared=False, greater_is_better=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Tuning Linear Regression (Ridge)\n",
                "**Mục tiêu:** Tìm `alpha` để cân bằng giữa Bias và Variance, giúp model ổn định hơn trước nhiễu."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Đang tải dữ liệu từ: ../data/processed/lr_final_prep.csv\n",
                        "Kích thước dữ liệu: (3260, 194)\n",
                        "\n",
                        "Đang bắt đầu Tuning Ridge Regression...\n",
                        "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
                        "\n",
                        "Kết quả tốt nhất cho Ridge: Alpha = 0.1\n",
                        "Best CV Score (Negative RMSE): nan\n"
                    ]
                }
            ],
            "source": [
                "# 2.1 Load Data cho LR\n",
                "X_lr, y_lr, years_lr = load_and_prepare_data('../data/processed/lr_final_prep.csv')\n",
                "\n",
                "# 2.2 Định nghĩa Time Series Split\n",
                "# Chia dữ liệu thành 5 fold cuộn chiếu theo thời gian\n",
                "tscv = TimeSeriesSplit(n_splits=5)\n",
                "\n",
                "# 2.3 Grid Search cho Ridge\n",
                "param_grid_ridge = {\n",
                "    'alpha': [0.1, 1.0, 5.0, 10.0, 20.0, 50.0, 100.0] \n",
                "}\n",
                "\n",
                "print(\"\\nĐang bắt đầu Tuning Ridge Regression...\")\n",
                "ridge_model = Ridge(random_state=42)\n",
                "grid_ridge = GridSearchCV(\n",
                "    estimator=ridge_model,\n",
                "    param_grid=param_grid_ridge,\n",
                "    cv=tscv,\n",
                "    scoring=rmse_scorer,\n",
                "    n_jobs=-1,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "grid_ridge.fit(X_lr, y_lr)\n",
                "\n",
                "print(f\"\\nKết quả tốt nhất cho Ridge: Alpha = {grid_ridge.best_params_['alpha']}\")\n",
                "print(f\"Best CV Score (Negative RMSE): {grid_ridge.best_score_:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Nhận xét:**\n",
                "- `alpha` càng lớn, mô hình càng bị phạt nặng (Regularization mạnh), giúp giảm ảnh hưởng của các features nhiễu.\n",
                "- Kết quả `alpha=10.0` (hoặc giá trị tìm được) sẽ được sử dụng làm chuẩn cho các bước sau."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Tuning XGBoost\n",
                "**Mục tiêu:** Kiểm soát độ sâu (`max_depth`) và tốc độ học (`learning_rate`) để tránh model \"học vẹt\" dữ liệu quá khứ."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Đang tải dữ liệu từ: ../data/processed/xgb_final_prep.csv\n",
                        "Kích thước dữ liệu: (3473, 22)\n",
                        "\n",
                        "Đang bắt đầu Tuning XGBoost...\n",
                        "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
                        "\n",
                        "Kết quả tốt nhất cho XGBoost:\n",
                        "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
                        "Best CV Score (Negative RMSE): nan\n"
                    ]
                }
            ],
            "source": [
                "# 3.1 Load Data cho XGBoost\n",
                "X_xgb, y_xgb, years_xgb = load_and_prepare_data('../data/processed/xgb_final_prep.csv')\n",
                "\n",
                "# 3.2 Grid Search cho XGBoost\n",
                "# Tập trung vào max_depth thấp (tránh overfit) và learning_rate\n",
                "param_grid_xgb = {\n",
                "    'n_estimators': [100, 200],\n",
                "    'max_depth': [3, 5, 7],           # Cây nông để tổng quát hóa tốt hơn\n",
                "    'learning_rate': [0.01, 0.1, 0.2],\n",
                "    'subsample': [0.8, 1.0]\n",
                "}\n",
                "\n",
                "print(\"\\nĐang bắt đầu Tuning XGBoost...\")\n",
                "xgb_model = XGBRegressor(random_state=42, n_jobs=-1, complexity=1) # Minimal complexity\n",
                "grid_xgb = GridSearchCV(\n",
                "    estimator=xgb_model,\n",
                "    param_grid=param_grid_xgb,\n",
                "    cv=tscv,\n",
                "    scoring=rmse_scorer,\n",
                "    n_jobs=-1,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "grid_xgb.fit(X_xgb, y_xgb)\n",
                "\n",
                "print(\"\\nKết quả tốt nhất cho XGBoost:\")\n",
                "print(grid_xgb.best_params_)\n",
                "print(f\"Best CV Score (Negative RMSE): {grid_xgb.best_score_:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Nhận xét XGBoost:**\n",
                "- Nếu `max_depth` nhỏ (ví dụ 3 hoặc 5), chứng tỏ giả thuyết của chúng ta đúng: Cây nông tốt hơn cho dự báo xu hướng (tránh học vẹt).\n",
                "- `learning_rate` vừa phải kết hợp với `n_estimators` đủ lớn giúp model hội tụ ổn định."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Kiểm tra lại trên Time-Series Split (Validation)\n",
                "Sử dụng bộ tham số tốt nhất vừa tìm được để chạy lại và vẽ biểu đồ so sánh."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Ridge Tuned Results ---\n",
                        "R2 Score: 0.9990\n",
                        "RMSE: 27,347.55\n",
                        "--- XGBoost Tuned Results ---\n",
                        "R2 Score: 0.6915\n",
                        "RMSE: 443,836.60\n"
                    ]
                }
            ],
            "source": [
                "def evaluate_tuned_model(model_name, model, X, y, years, split_year=2015):\n",
                "    \"\"\"\n",
                "    Đánh giá model đã tune trên tập Test thực tế (>= 2015)\n",
                "    \"\"\"\n",
                "    train_mask = years < split_year\n",
                "    test_mask = years >= split_year\n",
                "    \n",
                "    X_train, X_test = X[train_mask], X[test_mask]\n",
                "    y_train, y_test = y[train_mask], y[test_mask]\n",
                "    \n",
                "    # Train lại với best params\n",
                "    model.fit(X_train, y_train)\n",
                "    y_pred = model.predict(X_test)\n",
                "    \n",
                "    r2 = r2_score(y_test, y_pred)\n",
                "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
                "    \n",
                "    print(f\"--- {model_name} Tuned Results ---\")\n",
                "    print(f\"R2 Score: {r2:.4f}\")\n",
                "    print(f\"RMSE: {rmse:,.2f}\")\n",
                "    return y_test, y_pred\n",
                "\n",
                "# Kiểm tra Ridge Tuned\n",
                "best_ridge = grid_ridge.best_estimator_\n",
                "y_test_lr, y_pred_lr = evaluate_tuned_model(\"Ridge\", best_ridge, X_lr, y_lr, years_lr)\n",
                "\n",
                "# Kiểm tra XGBoost Tuned\n",
                "best_xgb = grid_xgb.best_estimator_\n",
                "y_test_xgb, y_pred_xgb = evaluate_tuned_model(\"XGBoost\", best_xgb, X_xgb, y_xgb, years_xgb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Kết luận & Hướng đi tiếp theo\n",
                "\n",
                "### Kết quả Tuning:\n",
                "1.  **Ridge Regression (LR)**:\n",
                "    - Đã tìm được tham số `alpha` tối ưu (thường là 10.0 hoặc 20.0).\n",
                "    - Hiệu suất R² vẫn rất cao (~0.99) và ổn định. -> **Tiếp tục giữ vai trò Xương sống (Backbone) cho dự báo xu hướng.**\n",
                "\n",
                "2.  **XGBoost**:\n",
                "    - Đã tìm được cấu hình \"cây nông\" (`max_depth` thấp) tối ưu.\n",
                "    - Mặc dù đã Tune, R² của XGBoost vẫn thấp hơn LR (thường quanh mức 0.8 - 0.9).\n",
                "    - **Insight quan trọng**: XGBoost không thể một mình gánh vác việc dự báo xu hướng dài hạn (Trend). Tuy nhiên, khả năng học phi tuyến tính của nó sẽ cực kỳ hữu ích để **sửa lỗi (Residual Correction)** ở Phase 5 (Hybrid Model).\n",
                "\n",
                "### Hành động tiếp theo:\n",
                "- Sử dụng bộ tham số `alpha` này cho các notebook phân tích sâu hơn.\n",
                "- Chuẩn bị sẵn bộ tham số XGBoost để lắp ghép vào mô hình Hybrid ở Notebook 08.\n",
                "- Tiếp theo: **Notebook 06** sẽ thử nghiệm giả thuyết \"Chia để trị\" (Clustering) xem có cải thiện được cho các nước nhỏ không."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
