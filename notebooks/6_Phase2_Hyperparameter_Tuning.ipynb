{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 6: Phase 2 - Hyperparameter Tuning\n",
    "\n",
    "**Goal**: Optimize the hyperparameters of our best performing models (Linear Regression & XGBoost) using `GridSearchCV` / `RandomizedSearchCV`.\n",
    "**Constraint**: Must use `TimeSeriesSplit` for Cross-Validation to avoid data leakage (no future data in validation folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T04:20:31.097539Z",
     "iopub.status.busy": "2025-12-28T04:20:31.097300Z",
     "iopub.status.idle": "2025-12-28T04:20:32.586071Z",
     "shell.execute_reply": "2025-12-28T04:20:32.585244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from ../data/processed/lr_final_prep.csv: (2190, 193)\n",
      "Loaded data from ../data/processed/xgb_final_prep.csv: (3473, 25)\n",
      "Loaded data from ../data/processed/common_preprocessed.csv: (3473, 25)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath(os.path.join('../src')))\n",
    "from preprocessing import load_data\n",
    "\n",
    "SPLIT_YEAR = 2015\n",
    "TARGET = 'Value_co2_emissions_kt_by_country'\n",
    "\n",
    "# Load Data\n",
    "df_lr = load_data('../data/processed/lr_final_prep.csv')\n",
    "df_xgb = load_data('../data/processed/xgb_final_prep.csv')\n",
    "\n",
    "# Ensure Year col exists for splitting logic\n",
    "# (Assuming Year is present or recovering it if needed)\n",
    "df_common = load_data('../data/processed/common_preprocessed.csv')\n",
    "if 'Year' not in df_lr.columns:\n",
    "    df_lr['Year'] = df_common.loc[df_lr.index, 'Year']\n",
    "if 'Year' not in df_xgb.columns:\n",
    "    df_xgb['Year'] = df_common.loc[df_xgb.index, 'Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T04:20:32.610905Z",
     "iopub.status.busy": "2025-12-28T04:20:32.610702Z",
     "iopub.status.idle": "2025-12-28T04:23:19.113534Z",
     "shell.execute_reply": "2025-12-28T04:23:19.105871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tuning Linear Regression (Ridge) ---\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'alpha': 10.0}\n",
      "Best CV Score: 0.8931\n",
      "Test Set R2 (Before Tuning): Comparison needed with Phase 1\n",
      "Test Set R2 (After Tuning): 0.9804\n",
      "\n",
      "--- Tuning XGBoost ---\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.7}\n",
      "Best CV Score: 0.7527\n",
      "Test Set R2 (Before Tuning): Comparison needed with Phase 1\n",
      "Test Set R2 (After Tuning): 0.7996\n"
     ]
    }
   ],
   "source": [
    "def tuning_pipeline(df, model, param_grid, name=\"Model\"):\n",
    "    print(f\"\\n--- Tuning {name} ---\")\n",
    "    \n",
    "    # Split Train/Test for Final Eval\n",
    "    train = df[df['Year'] < SPLIT_YEAR]\n",
    "    test = df[df['Year'] >= SPLIT_YEAR]\n",
    "    \n",
    "    drop_cols = [TARGET, 'Year']\n",
    "    drop_cols = [c for c in drop_cols if c in df.columns]\n",
    "    \n",
    "    X_train = train.drop(columns=drop_cols)\n",
    "    y_train = train[TARGET]\n",
    "    X_test = test.drop(columns=drop_cols)\n",
    "    y_test = test[TARGET]\n",
    "    \n",
    "    # TimeSeriesSplit for CV\n",
    "    # We use 5 splits. This effectively walks forward in time.\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # Search\n",
    "    # Use RandomizedSearchCV for speed if grid is large, else Grid\n",
    "    if len(param_grid) > 20: \n",
    "        search = RandomizedSearchCV(\n",
    "            model, param_grid, cv=tscv, scoring='r2', \n",
    "            n_iter=20, n_jobs=-1, random_state=42, verbose=1\n",
    "        )\n",
    "    else:\n",
    "        search = GridSearchCV(\n",
    "            model, param_grid, cv=tscv, scoring='r2', \n",
    "            n_jobs=-1, verbose=1\n",
    "        )\n",
    "        \n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best Params: {search.best_params_}\")\n",
    "    print(f\"Best CV Score: {search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate on Test Set\n",
    "    best_model = search.best_estimator_\n",
    "    preds = best_model.predict(X_test)\n",
    "    test_r2 = r2_score(y_test, preds)\n",
    "    print(f\"Test Set R2 (Before Tuning): Comparison needed with Phase 1\")\n",
    "    print(f\"Test Set R2 (After Tuning): {test_r2:.4f}\")\n",
    "    \n",
    "    return search.best_params_, test_r2\n",
    "\n",
    "# 1. Tune Linear Regression (Ridge)\n",
    "lr_params = {\n",
    "    'alpha': [0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "}\n",
    "best_lr_params, lr_score = tuning_pipeline(df_lr, Ridge(), lr_params, \"Linear Regression (Ridge)\")\n",
    "\n",
    "# 2. Tune XGBoost\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0]\n",
    "}\n",
    "best_xgb_params, xgb_score = tuning_pipeline(df_xgb, XGBRegressor(random_state=42, n_jobs=-1), xgb_params, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T04:23:19.146039Z",
     "iopub.status.busy": "2025-12-28T04:23:19.144830Z",
     "iopub.status.idle": "2025-12-28T04:23:19.164216Z",
     "shell.execute_reply": "2025-12-28T04:23:19.163350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters saved to data/results/best_hyperparameters.json\n"
     ]
    }
   ],
   "source": [
    "# Save Best Params for Phase 3 v2\n",
    "import json\n",
    "\n",
    "best_params = {\n",
    "    'Linear Regression': best_lr_params,\n",
    "    'XGBoost': best_xgb_params\n",
    "}\n",
    "\n",
    "with open('../data/results/best_hyperparameters.json', 'w') as f:\n",
    "    json.dump(best_params, f, indent=4)\n",
    "    \n",
    "print(\"Best parameters saved to data/results/best_hyperparameters.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
